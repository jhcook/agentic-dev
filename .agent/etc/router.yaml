version: 1.0

models:
  # --- OpenAI Models ---
  gpt-4o:
    provider: "openai"
    deployment_id: "gpt-4o"
    tier: "advanced"
    context_window: 128000
    cost_per_1k_input: 0.0025
    cost_per_1k_output: 0.0100
    supports_vision: true

  gpt-4o-mini:
    provider: "openai"
    deployment_id: "gpt-4o-mini"
    tier: "light"
    context_window: 128000
    cost_per_1k_input: 0.00015
    cost_per_1k_output: 0.00060

  # --- Gemini Models ---
  gemini-pro-latest:
    provider: "gemini"
    deployment_id: "gemini-pro-latest"
    tier: "advanced"
    context_window: 2000000
    cost_per_1k_input: 0.00125
    cost_per_1k_output: 0.00500

  gemini-1.5-flash:
    provider: "gemini"
    deployment_id: "gemini-2.0-flash"
    tier: "standard"
    context_window: 1000000
    cost_per_1k_input: 0.000075
    cost_per_1k_output: 0.000300

  # --- GitHub CLI ---
  gh-copilot:
    provider: "gh"
    deployment_id: "openai/gpt-4o"
    tier: "standard"
    context_window: 8000
    cost_per_1k_input: 0.0000
    cost_per_1k_output: 0.0000

  # --- Ollama (Self-hosted) ---
  ollama-llama3:
    provider: "ollama"
    base_url: "http://localhost:11434"
    deployment_id: "llama3"
    tier: "local"
    context_window: 8192
    cost_per_1k_input: 0.0000
    cost_per_1k_output: 0.0000

  ollama-codellama:
    provider: "ollama"
    base_url: "http://localhost:11434"
    deployment_id: "codellama"
    tier: "local"
    context_window: 16384
    cost_per_1k_input: 0.0000
    cost_per_1k_output: 0.0000

  # --- Anthropic Claude 4.5 Models ---
  claude-sonnet-4-5:
    provider: "anthropic"
    deployment_id: "claude-sonnet-4-5-20250929"
    tier: "advanced"
    context_window: 200000
    cost_per_1k_input: 0.003
    cost_per_1k_output: 0.015

  claude-haiku-4-5:
    provider: "anthropic"
    deployment_id: "claude-haiku-4-5-20250929"
    tier: "standard"
    context_window: 200000
    cost_per_1k_input: 0.001
    cost_per_1k_output: 0.005

  claude-opus-4-5:
    provider: "anthropic"
    deployment_id: "claude-opus-4-5-20250929"
    tier: "premium"
    context_window: 200000
    cost_per_1k_input: 0.015
    cost_per_1k_output: 0.075

settings:
  default_tier: "standard"
  provider_priority: ["gemini", "openai", "ollama", "gh"]
